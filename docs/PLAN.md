# Implementation Plan for AI Co-Scientist Hackathon Project

We will build an AI Co-Scientist application as part of the Agentic AI App Hackathon, leveraging Google Cloud Run with an NVIDIA L4 GPU per project.
The app will use Python (FastAPI for the backend) and Google's open-source Agent Development Kit (ADK) to orchestrate a multi-agent workflow.
In this system, a researcher's goal (provided in natural language) is passed through specialized agents that *generate, critique, and refine* novel scientific hypotheses and experimental plans, inspired by Google's AI co-scientist design.
We will integrate open models (e.g. Gemma 3 and Llama 2 on Cloud Run) and databases to store knowledge, following best practices for Cloud Run serverless GPUs and ADK agents.

## Multi-Agent System Design

Our AI co-scientist will use a multi-agent architecture. A Supervisor agent first parses the researcher's prompt into subtasks and assigns them to worker agents. Key agents include:

*   **Generation Agent:** Uses a large language model (LLM) to propose novel hypotheses or research ideas based on the goal.
*   **Reflection (Critic) Agent:** Reviews each hypothesis for plausibility, coherence, and novelty, possibly suggesting improvements.
*   **Ranking Agent:** Scores and ranks the generated ideas to select the most promising ones.
*   **Evolution Agent:** Iteratively refines top hypotheses by combining or mutating ideas (akin to "evolution" or self-play) to boost quality.
*   **Proximity Agent:** Optionally retrieves related knowledge (e.g. via search or embeddings) to ensure outputs are grounded and novel.
*   **Meta-Review Agent:** Performs a final check, ensuring the output is coherent and proposes experimental steps or proposals.

The diagram above (inspired by Google's research) illustrates this multi-agent flow: a user's research goal is fed into a Supervisor, which coordinates Generation, Reflection, Ranking, and Evolution agents in loops until high-quality hypotheses emerge. Each agent can use internal tools (e.g. calling a search function or model) to inform its decisions. We will use ADK's workflow constructs (such as Sequential and Loop agents) to implement this pipeline.

Each agent will be implemented using the **Google ADK** (Agent Development Kit). ADK is a modular Python framework (installable via `pip install google-adk`) that makes it easy to define LLM-driven agents and orchestrate them in workflows [google.github.io](http://google.github.io). ADK is model-agnostic and provides built-in tools (e.g. Google Search or code execution) and integration points (LangChain, other libraries) to enrich agent capabilities [google.github.io](http://google.github.io) [google.github.io](http://google.github.io). We will define a custom ADK "agent team" with the above roles, using ADK's memory/session services to track conversation state and long-term knowledge. For example, once a hypothesis is finalized, ADK's Memory Service can store it [vector store](http://vector store) or Firestore for future retrieval [google.github.io](http://google.github.io).

Agents will communicate through structured outputs: e.g. the Generation agent returns a list of hypotheses; the Reflection agent returns critiques or revisions; the Ranking agent returns scores. ADK's parallel and loop constructs allow us to perform tournaments or iterative loops (e.g. each hypothesis can be evaluated by multiple critics, and the best can be evolved further). This mirrors the "self-play" and Elo-ranking approach described in the co-scientist paper [research.google](http://research.google) [research.google](http://research.google), though for the hackathon prototype we will implement a simpler iterative refinement.

## Tech Stack and Infrastructure

*   **Language & Framework:** Language & Framework: Python backend (e.g. FastAPI or Flask) exposing REST/JSON API endpoints. Agents will be built with the Google ADK (Python) library [google.github.io](http://google.github.io). We'll containerize the app (including the ADK agent code and model inference code) using a Dockerfile.
*   **Models:** Models: We will use **lightweight open models** that fit on one L4 GPU. For text LLMs, candidates include Gemma 3 (4B or 12B) and Llama 2 (7B), which the hackathon supports [nyc.aitinkerers.org](http://nyc.aitinkerers.org). These models can be loaded via Hugging Face Transformers or Ollama. For image-related content (if needed, e.g. schematics), we can use **Stable Diffusion XL (SDXL)**. The chosen model(s) will be run on the L4 GPU in Cloud Run. Gemma 3 is optimized for single-GPU inference and supports long contexts [cloud.google.com](http://cloud.google.com).
*   **Deployment:** Deployment: We will deploy to **Google Cloud Run** (serverless) with one **NVIDIA L4 GPU** attached [nyc.aitinkerers.org](http://nyc.aitinkerers.org) [cloud.google.com](http://cloud.google.com). Cloud Run on L4 GPUs starts in ~5 seconds and scales to zero when idle [cloud.google.com](http://cloud.google.com), which suits iterative agent calls. Each Cloud Run service can host an agent pipeline. We may use one service with multiple endpoints, or separate services for "LLM API" versus "chat API". We will enable the Cloud Run API and grant roles (run.admin, etc.) as documented [cloud.google.com](http://cloud.google.com) [cloud.google.com](http://cloud.google.com).
*   **Inference Optimization:** Inference Optimization: To maximize throughput, we'll run inference in batching or use vLLM for async streaming if needed. We'll load models into GPU memory and reuse them across calls. We will consider quantization or FP16/BF16 for speed. We will also leverage Google Gen AI SDK patterns (e.g. "Vertex AI Express Mode" if needed) for integration, though ADK abstracts the model calls.
*   **Tools & Integrations:** Tools & Integrations: ADK provides built-in tools that agents can use. For example, the google_search tool (for Gemini models) or a custom API tool could allow an agent to fetch current scientific facts. We may also include a Python execution tool for simple computations. ADK can integrate third-party libraries (LangChain, CrewAI, etc.) [google.github.io](http://google.github.io). For data, we might connect to Google BigQuery or a vector database as a knowledge base.

## Data and Memory

We will use a database to store knowledge and session data. For long-term memory, we can use Vertex AI Matching Engine or Firestore as the ADK MemoryService backend [google.github.io](http://google.github.io). As agents run, key facts (e.g. past hypotheses, important citations) can be indexed. Then a Retrieval tool (ADK tool wrapping Vertex AI Search) can query this knowledge to ground new hypotheses. A simple approach: each session's final hypotheses are vectorized and stored; new sessions can recall similar ideas.

For the short term, ADK's Session service (in-memory or Redis) will track the conversation state (what ideas have been generated/edited). We'll persist final results and user feedback in a SQL/NoSQL DB (like Firestore or Cloud SQL) so the user can iterate across multiple API calls.

## Implementation Steps

1.  **Environment Setup:** Create a Google Cloud project, enable Cloud Run, GPU and AI APIs. Install ADK (pip install google-adk), and get LLM credentials or model files. Ensure permissions (roles/run.developer, roles/iam.serviceAccountUser) are granted [cloud.google.com](http://cloud.google.com/) [cloud.google.com](http://cloud.google.com/)
2.  **Basic Agent Prototype:** Start with a simple ADK Sequential agent: user goal → prompt a single LLM → return output. Test locally (no GPU).
3.  **Define Agent Team:** Extend to multiple agents. Write Python classes/functions for Generation, Reflection, and Ranking agents. For example, the Generation agent could prompt the LLM: "Given this goal, propose 5 novel research hypotheses." The Reflection agent could prompt: "Critique this hypothesis for viability and suggest improvements." Use ADK's LlmAgent or custom function tools.
4.  **Iterative Loop & Self-Critique:** Implement a loop or tournament: generate N hypotheses, have M agents critique or rank them, then optionally feed back to generation or evolution. Use ADK's Loop or Parallel agents to structure these stages [google.github.io](http://google.github.io/)
5.  **Model Integration:** Decide on specific LLM(s). For example, deploy Gemma 3 (4B) on a local L4 GPU using Hugging Face weights. Integrate the model calls in each agent (using google.genai or transformers). Ensure proper tokenization and output handling.
6.  **Database & Memory:** Set up a Firestore or Cloud SQL instance. Develop a MemoryService to write session summaries to DB and a tool for agents to query it. In ADK, register this memory and use it in relevant agents (e.g. Ranking agent could look up similar past ideas).
7.  **Tools & Search (Optional):** If needed for grounding, configure ADK's built-in Google Search tool (note: only with Gemini models) [google.github.io](http://google.github.io/) Alternatively, use a browser-based search API or pre-downloaded dataset of papers. Integrate a tool into an agent where it formulates a query and ingests results as context.
8.  **Backend API Development:** Create a Python server (FastAPI) with endpoints like /generate_ideas, /refine, /finalize. Each endpoint can invoke the ADK runner for the full multi-agent pipeline or for one step. Ensure JSON input/output (goal text, returned hypotheses, etc.).
9.  **Testing & Iteration:** Test the agents on sample research goals (e.g. "new antibiotics for bacteria X" or "AI methods for climate modeling"). Evaluate coherence and novelty. Debug prompts, increase/decrease number of agents or iterations as needed. Use the GPU to speed up LLM inference and measure latency.
10. **Containerization & Deployment:** Write a Dockerfile (e.g. based on ubuntu:22.04, install CUDA drivers are pre-installed on Cloud Run's L4 runtime [cloud.google.com](http://cloud.google.com/)). Copy app code and requirements, expose port 8080. Deploy to Cloud Run with gcloud run deploy, selecting 4 vCPU and 16GB memory (required minimum for 1 L4 GPU [cloud.google.com](http://cloud.google.com/)) and attach one NVIDIA L4 GPU. Configure concurrency=1 (to maximize GPU utilization).
11. **Optimization:** Once deployed, test cold start (should be ~5s) and inference. If too slow, switch to a smaller model or reduce context. Consider using Google's prebuilt Gemma container or Vertex AI for managed hosting [cloud.google.com](http://cloud.google.com)
12. **Demo Prep:** Build a simple front-end or use curl requests to demonstrate the system: show input goal, intermediate outputs (proposed hypotheses), and final ranked proposals. Document the setup.

Throughout development, we will follow best practices for Cloud Run and GPUs (e.g. cost monitoring, only 1 GPU per project) [cloud.google.com](http://cloud.google.com) We will also leverage mentors' guidance and sample ADK code. By the hackathon deadline, we aim to have a working end-to-end prototype that, given a scientific question, outputs novel research directions, showcasing the agentic AI approach [research.google](http://research.google) [cloud.google.com](http://cloud.google.com)

## Sources

We based this plan on Google's AI Co-Scientist research and the hackathon guidelines, including the use of ADK and Cloud Run GPUs [research.google](http://research.google) [nyc.aitinkerers.org](http://nyc.aitinkerers.org) [cloud.google.com](http://cloud.google.com) [cloud.google.com](http://cloud.google.com) The ADK documentation provides guidance on agents, tools, and memory services [google.github.io](http://google.github.io) [google.github.io](http://google.github.io) [google.github.io](http://google.github.io) and Google Cloud blogs detail deploying models on Cloud Run with L4 GPUs [cloud.google.com](http://cloud.google.com) [cloud.google.com](http://cloud.google.com) All specific requirements (models, environment, GPUs) come from the official hackathon handbook and Google sources [nyc.aitinkerers.org](http://nyc.aitinkerers.org) [d.google.com](http://d.google.com)
